{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 8: Model-free (RL) Prediction With Monte Carlo and Temporal Difference\n",
    "### 1. Monte-Carlo\n",
    "** Class Design: My class design of Monte-Carlo is based on a interface for tabular RL algorithms. It includes both the first-time visit and every-time visit Monte-Carlo Alogorithm**\n",
    "\n",
    "(1) First-visit Monte-Carlo\n",
    "\n",
    "<img src=\"first_visit_montecarlo.png\">\n",
    "\n",
    "(2) Every-visit Monte-Carlo\n",
    "\n",
    "<img src=\"every_visit_montecarlo.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the Monte Carlo Paths: \n",
      "[(1, 'a', 4.5, True), (2, 'a', 9.8, True), (1, 'b', 2.6, False), (3, 'End', 0, 'End')]\n",
      "Estimated Value Function: \n",
      "{1: 12.188085937500066, 2: 17.262925851703432, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from src.monte_carlo import MonteCarlo\n",
    "from src.mdp_refined import MDPRefined\n",
    "\n",
    "mdp_refined_data = {\n",
    "        1: {\n",
    "            'a': {1: (0.3, 9.2), 2: (0.6, 4.5), 3: (0.1, 5.0)},\n",
    "            'b': {2: (0.3, -0.5), 3: (0.7, 2.6)},\n",
    "            'c': {1: (0.2, 4.8), 2: (0.4, -4.9), 3: (0.4, 0.0)}\n",
    "        },\n",
    "        2: {\n",
    "            'a': {1: (0.3, 9.8), 2: (0.6, 6.7), 3: (0.1, 1.8)},\n",
    "            'c': {1: (0.2, 4.8), 2: (0.4, 9.2), 3: (0.4, -8.2)}\n",
    "        },\n",
    "        3: {\n",
    "            'a': {3: (1.0, 0.0)},\n",
    "            'b': {3: (1.0, 0.0)}\n",
    "        }\n",
    "    }\n",
    "\n",
    "gamma_val = 1.0\n",
    "mdp_ref_obj1 = MDPRefined(mdp_refined_data, gamma_val)\n",
    "mdp_rep_obj = mdp_ref_obj1.get_mdp_rep_for_rl_tabular()\n",
    "\n",
    "exploring_start_val = False\n",
    "first_visit_flag = True\n",
    "episodes_limit = 1000\n",
    "max_steps_val = 1000\n",
    "mc_obj = MonteCarlo(mdp_rep_obj,exploring_start_val,first_visit_flag,episodes_limit,max_steps_val)\n",
    "\n",
    "policy_data = {\n",
    "        1: {'a': 0.4, 'b': 0.6},\n",
    "        2: {'a': 0.7, 'c': 0.3},\n",
    "        3: {'b': 1.0}\n",
    "    }\n",
    "\n",
    "this_mc_path = mc_obj.get_mc_path(policy_data, 1)\n",
    "print(\"One of the Monte Carlo Paths: \")\n",
    "print(this_mc_path)\n",
    "\n",
    "this_vf_dict = mc_obj.get_value_func_dict(policy_data)\n",
    "print(\"Estimated Value Function: \")\n",
    "print(this_vf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Temporal-Difference Learning\n",
    "\n",
    "<img src=\"TD0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function by TD0: \n",
      "{1: 10.704335322963122, 2: 13.415799208176635, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from src.td0 import TD0\n",
    "\n",
    "exploring_start_val = False\n",
    "epsilon_val = 0.1\n",
    "epsilon_half_life_val = 1000\n",
    "learning_rate_val = 0.1\n",
    "learning_rate_decay_val = 1e6\n",
    "episodes_limit = 10000\n",
    "max_steps_val = 1000\n",
    "sarsa_obj = TD0(\n",
    "        mdp_rep_obj,\n",
    "        exploring_start_val,\n",
    "        epsilon_val,\n",
    "        epsilon_half_life_val,\n",
    "        learning_rate_val,\n",
    "        learning_rate_decay_val,\n",
    "        episodes_limit,\n",
    "        max_steps_val\n",
    "    )\n",
    "\n",
    "this_qf_dict = sarsa_obj.get_value_func_dict(policy_data)\n",
    "print(\"Estimated Value Function by TD0: \")\n",
    "print(this_qf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test it against VI and PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation: \n",
      "{1: 13.136856554564174, 2: 19.462937542896363}\n"
     ]
    }
   ],
   "source": [
    "val = mdp_ref_obj1.policy_evaluation(policy_data)\n",
    "print(\"Policy Evaluation: \")\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prove that fixed learning rate (step size alpha) for MC is equivalent to an exponentially decaying average of episode returns\n",
    "\n",
    "Denote $V\\left(S_{t}\\right)^{k}$  as the kth update of $V\\left(S_{t}\\right)$:\n",
    "$$\n",
    "\\begin{array}{c}{V\\left(S_{t}\\right)^{k}=V\\left(S_{t}\\right)^{k-1}+\\alpha\\left(G_{t}^{k-1}-V\\left(S_{t}\\right)^{k-1}\\right)=(1-\\alpha) V\\left(S_{t}\\right)^{k-1}+\\alpha^{*} G_{t}^{k-1}} \\\\ {V\\left(S_{t}\\right)^{k}=(1-\\alpha)^{*}\\left((1-\\alpha) V\\left(S_{t}\\right)^{k-2}+\\alpha^{*} G_{t}^{k-2}\\right)+\\alpha^{*} G_{t}^{k-1}}\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{c}{V\\left(S_{t}\\right)^{k}=(1-\\alpha)^{2} V\\left(S_{t}\\right)^{k-2}+(1-\\alpha)^{*} \\alpha^{*} G_{t}^{k-2}+\\alpha^{*} G_{t}^{k-1}} \\\\ {V\\left(S_{t}\\right)^{k}=(1-\\alpha)^{k-1} * \\alpha^{*} G_{t}^{0}+\\ldots+(1-\\alpha)^{*} \\alpha^{*} G_{I}^{k-2}+\\alpha^{*} G_{t}^{k-1}} \\\\ {V\\left(S_{t}\\right)^{k}=\\alpha\\left((1-\\alpha)^{k-1} * G_{t}^{0}+\\ldots+(1-\\alpha) * G_{t}^{k-2}+\\alpha^{*} G_{t}^{k-1}\\right)}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
