{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: Markov Decision Processes (MDP), Value Function, and Bellman Equations\n",
    "\n",
    "### Bellman equation for MRP Value Function\n",
    "**1. Bellman equation: ** $v = R + \\gamma P v$\n",
    "\n",
    "**2. Solve by Matrix Inversion: ** $v = (I - \\gamma P)^{-1}R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-terminal states:\n",
      "{1, 2, 3}\n",
      "The transition matrix without termial states:\n",
      "[[0.6 0.3]\n",
      " [0.1 0.2]]\n",
      "Bellman equation solution for MRP value function: \n",
      "[29.65517241 16.20689655]\n"
     ]
    }
   ],
   "source": [
    "from src.mrp import MRP\n",
    "\n",
    "transitions = {\n",
    "        1: {1: 0.6, 2: 0.3, 3: 0.1}, \n",
    "        2: {1: 0.1, 2: 0.2, 3: 0.7},\n",
    "        3: {3: 1.0}\n",
    "    }\n",
    "reward = {1: 7.0, 2:10.0, 3:0.0}\n",
    "gamma = 1.0\n",
    "mrp_obj = MRP(transitions, reward, gamma)\n",
    "print(\"Non-terminal states:\")\n",
    "print(mrp_obj.get_states())\n",
    "print(\"The transition matrix without termial states:\")\n",
    "print(mrp_obj.get_trans_matrix())\n",
    "print(\"Bellman equation solution for MRP value function: \")\n",
    "print(mrp_obj.valueFun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "**1. Definition of MDP: ** A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov. It can be represented as a tuple $\\langle \\mathcal { S } , \\mathcal { A } , \\mathcal { P } , \\mathcal { R } , \\gamma \\rangle$. In addition to MRP, $A$ is a finite set of actions. $\\mathcal { P } _ { s s ^ { \\prime } } ^ { a } = \\mathbb { P } \\left[ S _ { t + 1 } = s ^ { \\prime } | S _ { t } = s , A _ { t } = a \\right]$ is a state transition probability matrix. $\\mathcal { R } _ { s } ^ { a } = \\mathbb { E } \\left[ R _ { t + 1 } | S _ { t } = s , A _ { t } = a \\right]$ is a reward function.\n",
    "\n",
    "**2. Policy: ** A policy $\\pi$ is a distribution over actions given states. $\\pi ( a | s ) = \\mathbb { P } \\left[ A _ { t } = a | S _ { t } = s \\right]$.\n",
    "\n",
    "**3. Value function: ** \n",
    "\n",
    "   (1)  The state-value function: $v _ { \\pi } ( s ) = \\mathbb { E } _ { \\pi } \\left[ G _ { t } | S _ { t } = s \\right]$ is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "   \n",
    "   (2)  The action-value function: $q _ { \\pi } ( s , a ) = \\mathbb { E } _ { \\pi } \\left[ G _ { t } | S _ { t } = s , A _ { t } = a \\right]$ is the expected return\n",
    "starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "\n",
    "**4. Bellman equation: ** \n",
    "\n",
    "   (1) $v _ { \\pi } ( s ) = \\sum _ { a \\in \\mathcal { A } } \\pi ( a | s ) q _ { \\pi } ( s , a )$\n",
    "   \n",
    "   (2) $q _ { \\pi } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } \\pi } ^ { a } v _ { \\pi } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (3) $v _ { \\pi } ( s ) = \\sum _ { a \\in \\mathcal { A } } \\pi ( a | s ) \\left( \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { \\pi } \\left( s ^ { \\prime } \\right) \\right)$\n",
    "   \n",
    "   (4) $q _ { \\pi } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } \\sum _ { a ^ { \\prime } \\in \\mathcal { A } } \\pi \\left( a ^ { \\prime } | s ^ { \\prime } \\right) q _ { \\pi } \\left( s ^ { \\prime } , a ^ { \\prime } \\right)$\n",
    "   \n",
    "   (5) $v _ { * } ( s ) = \\max _ { a } q _ { * } ( s , a )$\n",
    "   \n",
    "   (6) $q _ { * } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { * } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (7) $v _ { * } ( s ) = \\max _ { a } \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { * } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (8) $q _ { * } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } \\max _ { a ^ { \\prime } } q _ { * } \\left( s ^ { \\prime } , a ^ { \\prime } \\right)$\n",
    "   \n",
    "   \n",
    "### Class Design for MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
