{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: Markov Decision Processes (MDP), Value Function, and Bellman Equations\n",
    "\n",
    "### Bellman equation for MRP Value Function\n",
    "**1. Bellman equation: ** $v = R + \\gamma P v$\n",
    "\n",
    "**2. Solve by Matrix Inversion: ** $v = (I - \\gamma P)^{-1}R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-terminal states:\n",
      "{1, 2, 3}\n",
      "The transition matrix without termial states:\n",
      "[[0.6 0.3]\n",
      " [0.1 0.2]]\n",
      "Bellman equation solution for MRP value function: \n",
      "[29.65517241 16.20689655]\n"
     ]
    }
   ],
   "source": [
    "from src.mrp import MRP\n",
    "\n",
    "transitions = {\n",
    "        1: {1: 0.6, 2: 0.3, 3: 0.1}, \n",
    "        2: {1: 0.1, 2: 0.2, 3: 0.7},\n",
    "        3: {3: 1.0}\n",
    "    }\n",
    "reward = {1: 7.0, 2:10.0, 3:0.0}\n",
    "gamma = 1.0\n",
    "mrp_obj = MRP(transitions, reward, gamma)\n",
    "print(\"Non-terminal states:\")\n",
    "print(mrp_obj.get_states())\n",
    "print(\"The transition matrix without termial states:\")\n",
    "print(mrp_obj.get_trans_matrix())\n",
    "print(\"Bellman equation solution for MRP value function: \")\n",
    "print(mrp_obj.valueFun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process\n",
    "**1. Definition of MDP: ** A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov. It can be represented as a tuple $\\langle \\mathcal { S } , \\mathcal { A } , \\mathcal { P } , \\mathcal { R } , \\gamma \\rangle$. In addition to MRP, $A$ is a finite set of actions. $\\mathcal { P } _ { s s ^ { \\prime } } ^ { a } = \\mathbb { P } \\left[ S _ { t + 1 } = s ^ { \\prime } | S _ { t } = s , A _ { t } = a \\right]$ is a state transition probability matrix. $\\mathcal { R } _ { s } ^ { a } = \\mathbb { E } \\left[ R _ { t + 1 } | S _ { t } = s , A _ { t } = a \\right]$ is a reward function.\n",
    "\n",
    "**2. Policy: ** A policy $\\pi$ is a distribution over actions given states. $\\pi ( a | s ) = \\mathbb { P } \\left[ A _ { t } = a | S _ { t } = s \\right]$.\n",
    "\n",
    "**3. Value function: ** \n",
    "\n",
    "   (1)  The state-value function: $v _ { \\pi } ( s ) = \\mathbb { E } _ { \\pi } \\left[ G _ { t } | S _ { t } = s \\right]$ is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "   \n",
    "   (2)  The action-value function: $q _ { \\pi } ( s , a ) = \\mathbb { E } _ { \\pi } \\left[ G _ { t } | S _ { t } = s , A _ { t } = a \\right]$ is the expected return\n",
    "starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "\n",
    "**4. Bellman equation: ** \n",
    "\n",
    "   (1) $v _ { \\pi } ( s ) = \\sum _ { a \\in \\mathcal { A } } \\pi ( a | s ) q _ { \\pi } ( s , a )$\n",
    "   \n",
    "   (2) $q _ { \\pi } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } \\pi } ^ { a } v _ { \\pi } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (3) $v _ { \\pi } ( s ) = \\sum _ { a \\in \\mathcal { A } } \\pi ( a | s ) \\left( \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { \\pi } \\left( s ^ { \\prime } \\right) \\right)$\n",
    "   \n",
    "   (4) $q _ { \\pi } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } \\sum _ { a ^ { \\prime } \\in \\mathcal { A } } \\pi \\left( a ^ { \\prime } | s ^ { \\prime } \\right) q _ { \\pi } \\left( s ^ { \\prime } , a ^ { \\prime } \\right)$\n",
    "   \n",
    "   (5) $v _ { * } ( s ) = \\max _ { a } q _ { * } ( s , a )$\n",
    "   \n",
    "   (6) $q _ { * } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { * } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (7) $v _ { * } ( s ) = \\max _ { a } \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { * } \\left( s ^ { \\prime } \\right)$\n",
    "   \n",
    "   (8) $q _ { * } ( s , a ) = \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } \\max _ { a ^ { \\prime } } q _ { * } \\left( s ^ { \\prime } , a ^ { \\prime } \\right)$\n",
    "   \n",
    "   \n",
    "### Class Design for MDP\n",
    "\n",
    "**1. Transform to MRP: **\n",
    "\n",
    "$Pr_{ss'} = \\sum_{a} \\pi(a|s)Pr_{ss'}^a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MDP process and reward: \n",
      "{1: {'a': {1: 0.2, 2: 0.6, 3: 0.2}, 'b': {1: 0.6, 2: 0.3, 3: 0.1}, 'c': {1: 0.1, 2: 0.2, 3: 0.7}}, 2: {'a': {1: 0.1, 2: 0.6, 3: 0.3}, 'c': {1: 0.6, 2: 0.2, 3: 0.2}}, 3: {'b': {3: 1.0}}}\n",
      "{1: {'a': 7.0, 'b': -2.0, 'c': 10.0}, 2: {'a': 1.0, 'c': -1.2}, 3: {'b': 0.0}}\n",
      "The MRP process and reward given a policy: \n",
      "{1: {1: 0.44, 2: 0.42, 3: 0.14}, 2: {1: 0.25, 2: 0.48, 3: 0.27}, 3: {3: 1.0}}\n",
      "{1: 1.6000000000000003, 2: 0.33999999999999997, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "from src.mdp import MDP\n",
    "mdp_data = {\n",
    "        1: {\n",
    "            'a': {1: 0.2, 2: 0.6, 3: 0.2},\n",
    "            'b': {1: 0.6, 2: 0.3, 3: 0.1},\n",
    "            'c': {1: 0.1, 2: 0.2, 3: 0.7}\n",
    "        },\n",
    "        2: {\n",
    "            'a': {1: 0.1, 2: 0.6, 3: 0.3},\n",
    "            'c': {1: 0.6, 2: 0.2, 3: 0.2}\n",
    "        },\n",
    "        3: {\n",
    "            'b': {3: 1.0}\n",
    "        }\n",
    "    }\n",
    "\n",
    "reward = {\n",
    "        1: {\n",
    "            'a': 7.0,\n",
    "            'b': -2.0,\n",
    "            'c': 10.0\n",
    "        },\n",
    "        2: {\n",
    "            'a': 1.0,\n",
    "            'c': -1.2\n",
    "        },\n",
    "        3: {\n",
    "            'b':  0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "policy_data = {\n",
    "        1: {'a': 0.4, 'b': 0.6},\n",
    "        2: {'a': 0.7, 'c': 0.3},\n",
    "        3: {'b': 1.0}\n",
    "    }\n",
    "\n",
    "gamma = 1\n",
    "mdp = MDP(mdp_data, reward, gamma)\n",
    "print(\"The MDP process and reward: \")\n",
    "print(mdp.process)\n",
    "print(mdp.state_reward)\n",
    "mrp = mdp.get_mrp(policy_data)\n",
    "print(\"The MRP process and reward given a policy: \")\n",
    "print(mrp.process)\n",
    "print(mrp.state_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Convert from a different definition: **\n",
    "\n",
    "$R(s,a) = \\sum_{s'} Pr_{ss'}^a r(s,s',a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'a': 7.0, 'b': -2.0, 'c': 1.0}, 2: {'a': 1.0, 'c': -1.2000000000000002}, 3: {'b': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from src.my_funcs import SASf, SAf\n",
    "\n",
    "def cast(rss: SASf, pr: SASf) -> SAf:\n",
    "    rs = {}\n",
    "    for state in pr.keys():\n",
    "        asf_pr = pr[state]\n",
    "        asf_rss = rss[state]\n",
    "        rs[state] = {}\n",
    "        for action, p in asf_pr.items():\n",
    "            rs[state][action] = 0\n",
    "            for s in p.keys():\n",
    "                rs[state][action] += p[s]*asf_rss[action][s]\n",
    "                \n",
    "    return rs\n",
    "\n",
    "mdp_data = {\n",
    "        1: {\n",
    "            'a': {1: 0.2, 2: 0.6, 3: 0.2},\n",
    "            'b': {1: 0.6, 2: 0.3, 3: 0.1},\n",
    "            'c': {1: 0.1, 2: 0.2, 3: 0.7}\n",
    "        },\n",
    "        2: {\n",
    "            'a': {1: 0.1, 2: 0.6, 3: 0.3},\n",
    "            'c': {1: 0.6, 2: 0.2, 3: 0.2}\n",
    "        },\n",
    "        3: {\n",
    "            'b': {3: 1.0}\n",
    "        }\n",
    "    }\n",
    "\n",
    "reward1 = {\n",
    "        1: {\n",
    "            'a': 7.0,\n",
    "            'b': -2.0,\n",
    "            'c': 10.0\n",
    "        },\n",
    "        2: {\n",
    "            'a': 1.0,\n",
    "            'c': -1.2\n",
    "        },\n",
    "        3: {\n",
    "            'b':  0.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "reward2 = {\n",
    "        1: {\n",
    "            'a': {1: 10, 2: 10, 3: -5},\n",
    "            'b': {1: -10, 2: 10, 3: 10},\n",
    "            'c': {1: 10, 2: 0, 3: 0}\n",
    "        },\n",
    "        2: {\n",
    "            'a': {1: 10, 2: 0, 3: 0},\n",
    "            'c': {1: 0, 2: -6, 3: 0}\n",
    "        },\n",
    "        3: {\n",
    "            'b': {3: 0}\n",
    "        }\n",
    "    }\n",
    "\n",
    "reward_cast = cast(mdp_data, reward2)\n",
    "print(reward_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
