{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 11: RL with Function Approximation\n",
    "### 1. Class Design of MonteCarlo\n",
    "$$\n",
    "\\Delta \\mathbf{w}=\\alpha\\left(G_{t}-\\hat{v}\\left(S_{t}, \\mathbf{w}\\right)\\right) \\nabla_{\\mathbf{w}} \\hat{v}\\left(S_{t}, \\mathbf{w}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.my_funcs import MDPRepForRLFA, SAf, get_nt_return_eval_steps, S, A, get_returns_from_rewards_terminating\n",
    "\n",
    "class MonteCarlo():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mdp_rep_for_rl: MDPRepForRLFA,\n",
    "        epsilon: float,\n",
    "        epsilon_half_life: float,\n",
    "        num_episodes: int,\n",
    "        max_steps: int,\n",
    "        fa_spec: FuncApproxSpec\n",
    "    ) -> None:\n",
    "\n",
    "        self.mdp_rep = mdp_rep_for_rl\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_half_life = epsilon_half_life\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.fa_spec = fa_spec\n",
    "        self.nt_return_eval_steps = get_nt_return_eval_steps(\n",
    "            max_steps,\n",
    "            mdp_rep.gamma,\n",
    "            1e-4\n",
    "        )\n",
    "\n",
    "    def get_mc_path(\n",
    "        self,\n",
    "        polf: PolicyActDictType,\n",
    "        start_state: S,\n",
    "        start_action: Optional[A] = None\n",
    "    ) -> Sequence[Tuple[S, A, float]]:\n",
    "\n",
    "        res = []\n",
    "        state = start_state\n",
    "        steps = 0\n",
    "        terminate = False\n",
    "\n",
    "        while not terminate:\n",
    "            action = get_rv_gen_func_single(polf(state))()\\\n",
    "                if (steps > 0 or start_action is None) else start_action\n",
    "            next_state, reward =\\\n",
    "                self.mdp_rep.state_reward_gen_func(state, action)\n",
    "            res.append((state, action, reward))\n",
    "            steps += 1\n",
    "            terminate = steps >= self.max_steps or\\\n",
    "                self.mdp_rep.terminal_state_func(state)\n",
    "            state = next_state\n",
    "        return res\n",
    "    \n",
    "    def get_value_func_fa(self, polf: PolicyActDictType) -> VFType:\n",
    "        episodes = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            start_state = self.mdp_rep.init_state_gen()\n",
    "            mc_path = self.get_mc_path(\n",
    "                polf,\n",
    "                start_state,\n",
    "                start_action=None\n",
    "            )\n",
    "\n",
    "            rew_arr = np.array([x for _, _, x in mc_path])\n",
    "            if self.mdp_rep.terminal_state_func(mc_path[-1][0]):\n",
    "                returns = get_returns_from_rewards_terminating(\n",
    "                    rew_arr,\n",
    "                    self.mdp_rep.gamma\n",
    "                )\n",
    "            else:\n",
    "                returns = get_returns_from_rewards_non_terminating(\n",
    "                    rew_arr,\n",
    "                    self.mdp_rep.gamma,\n",
    "                    self.nt_return_eval_steps\n",
    "                )\n",
    "\n",
    "            sgd_pts = [(mc_path[i][0], r) for i, r in enumerate(returns)]\n",
    "            self.vf_fa.update_params(*zip(*sgd_pts))\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "        return self.vf_fa.get_func_eval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def FuncApproxSpec():\n",
    "    \n",
    "    def __init__(func_spec: Callable):\n",
    "        self.func = func_spec\n",
    "        pass\n",
    "    \n",
    "    def gradient(self, x, para):\n",
    "        pass\n",
    "    \n",
    "    def update_params():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Class Design of 1-step TD Prediction\n",
    "$$\n",
    "\\Delta \\mathbf{w}=\\alpha\\left(R_{t+1}+\\gamma \\hat{v}\\left(S_{t+1}, \\mathbf{w}\\right)-\\hat{v}\\left(S_{t}, \\mathbf{w}\\right)\\right) \\nabla_{\\mathbf{w}} \\hat{v}\\left(S_{t}, \\mathbf{w}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD0():\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            mdp_rep_for_rl: MDPRepForRLFA,\n",
    "            exploring_start: bool,\n",
    "            algorithm: TDAlgorithm,\n",
    "            softmax: bool,\n",
    "            epsilon: float,\n",
    "            epsilon_half_life: float,\n",
    "            num_episodes: int,\n",
    "            max_steps: int,\n",
    "            fa_spec: FuncApproxSpec\n",
    "    ) -> None:\n",
    "\n",
    "        self.mdp_rep_for_rl=mdp_rep_for_rl,\n",
    "        self.exploring_start=exploring_start,\n",
    "        self.softmax=softmax,\n",
    "        self.epsilon=epsilon,\n",
    "        self.epsilon_half_life=epsilon_half_life,\n",
    "        self.num_episodes=num_episodes,\n",
    "        self.max_steps=max_steps,\n",
    "        self.fa_spec=fa_spec\n",
    "        self.algorithm: TDAlgorithm = algorithm\n",
    "\n",
    "    def get_value_func_fa(self, polf: PolicyActDictType) -> VFType:\n",
    "        episodes = 0\n",
    "\n",
    "        while episodes < self.num_episodes:\n",
    "            state = self.mdp_rep.init_state_gen()\n",
    "            steps = 0\n",
    "            terminate = False\n",
    "\n",
    "            while not terminate:\n",
    "                action = get_rv_gen_func_single(polf(state))()\n",
    "                next_state, reward = \\\n",
    "                    self.mdp_rep.state_reward_gen_func(state, action)\n",
    "                target = reward + self.mdp_rep.gamma *\\\n",
    "                    self.vf_fa.get_func_eval(next_state)\n",
    "                self.vf_fa.update_params([state], [target])\n",
    "                steps += 1\n",
    "                terminate = steps >= self.max_steps or \\\n",
    "                    self.mdp_rep.terminal_state_func(state)\n",
    "                state = next_state\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "        return self.vf_fa.get_func_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
