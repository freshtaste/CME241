{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 10: Model-free Control (RL for Optimal Value Function/Policy)\n",
    "## 1. Prove the Epsilon-Greedy Policy Improvement Theorem \n",
    "** Theorem: ** For any $\\epsilon$-greedy policy $\\pi$, the $\\epsilon$-greedy policy $\\pi^{\\prime}$ with respect to $q_{\\pi}$ is an improvement, $v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s)$.\n",
    "** Proof: ** \n",
    "$$\n",
    "\\begin{aligned} q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) &=\\sum_{a \\in \\mathcal{A}} \\pi^{\\prime}(a | s) q_{\\pi}(s, a) \\\\ &=\\epsilon / m \\sum_{a \\in \\mathcal{A}} q_{\\pi}(s, a)+(1-\\epsilon) \\max _{a \\in \\mathcal{A}} q_{\\pi}(s, a) \\\\ & \\geq \\epsilon / m \\sum_{a \\in \\mathcal{A}} q_{\\pi}(s, a)+(1-\\epsilon) \\sum_{a \\in \\mathcal{A}} \\frac{\\pi(a | s)-\\epsilon / m}{1-\\epsilon} q_{\\pi}(s, a) \\\\ &=\\sum_{a \\in \\mathcal{A}} \\pi(a | s) q_{\\pi}(s, a)=v_{\\pi}(s) \\end{aligned}\n",
    "$$\n",
    "Therefore from policy improvement theorem, $v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s)$.\n",
    "\n",
    "## 2. Provide the defintion of GLIE (Greedy in the Limit with Infinite Exploration)\n",
    "** Definition: **\n",
    "\n",
    "   (1) All state-action pairs are explored infinitely many times\n",
    "   \n",
    "$$\n",
    "\\lim _{k \\rightarrow \\infty} N_{k}(s, a)=\\infty\n",
    "$$\n",
    "   (2) The policy converges on a greedy policy\n",
    " \n",
    "$$\n",
    "\\lim _{k \\rightarrow \\infty} \\pi_{k}(a | s)=1\\left(a=\\underset{a^{\\prime} \\in \\mathcal{A}}{\\operatorname{argmax}} Q_{k}\\left(s, a^{\\prime}\\right)\\right)\n",
    "$$\n",
    "\n",
    "## 3. Implement the tabular SARSA and tabular SARSA(Lambda) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-action function estimates with Sarsa\n",
      "{1: {'b': 10.883485571927277, 'a': 29.01505207860538, 'c': 13.090263933704076}, 2: {'a': 29.128953572925912, 'c': 10.200608175810016}, 3: {'b': 0.0, 'a': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "from src.td0_control import TD0_control\n",
    "from src.mdp_refined import MDPRefined\n",
    "\n",
    "mdp_refined_data = {\n",
    "        1: {\n",
    "            'a': {1: (0.3, 9.2), 2: (0.6, 4.5), 3: (0.1, 5.0)},\n",
    "            'b': {2: (0.3, -0.5), 3: (0.7, 2.6)},\n",
    "            'c': {1: (0.2, 4.8), 2: (0.4, -4.9), 3: (0.4, 0.0)}\n",
    "        },\n",
    "        2: {\n",
    "            'a': {1: (0.3, 9.8), 2: (0.6, 6.7), 3: (0.1, 1.8)},\n",
    "            'c': {1: (0.2, 4.8), 2: (0.4, 9.2), 3: (0.4, -8.2)}\n",
    "        },\n",
    "        3: {\n",
    "            'a': {3: (1.0, 0.0)},\n",
    "            'b': {3: (1.0, 0.0)}\n",
    "        }\n",
    "    }\n",
    "gamma_val = 0.9\n",
    "mdp_ref_obj1 = MDPRefined(mdp_refined_data, gamma_val)\n",
    "mdp_rep_obj = mdp_ref_obj1.get_mdp_rep_for_rl_tabular()\n",
    "epsilon_val = 0.1\n",
    "epsilon_half_life_val = 100\n",
    "learning_rate_val = 0.1\n",
    "learning_rate_decay_val = 1e6\n",
    "episodes_limit = 5000\n",
    "max_steps_val = 1000\n",
    "\n",
    "sarsa = TD0_control(\n",
    "        mdp_rep_obj,\n",
    "        epsilon_val,\n",
    "        epsilon_half_life_val,\n",
    "        learning_rate_val,\n",
    "        learning_rate_decay_val,\n",
    "        episodes_limit,\n",
    "        max_steps_val,\n",
    "        'Sarsa'\n",
    "    )\n",
    "\n",
    "print(\"Value-action function estimates with Sarsa\")\n",
    "qv_sarsa = sarsa.get_qv_func_dict()\n",
    "print(qv_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement the tabular Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-action function estimates with Q-learning\n",
      "{1: {'b': 11.606916520154726, 'a': 35.42292975972827, 'c': 17.902709119148522}, 2: {'a': 34.068557603582164, 'c': 18.34462477171524}, 3: {'b': 0.0, 'a': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "qlearning = TD0_control(\n",
    "        mdp_rep_obj,\n",
    "        epsilon_val,\n",
    "        epsilon_half_life_val,\n",
    "        learning_rate_val,\n",
    "        learning_rate_decay_val,\n",
    "        episodes_limit,\n",
    "        max_steps_val,\n",
    "        'Q-learning'\n",
    "    )\n",
    "\n",
    "print(\"Value-action function estimates with Q-learning\")\n",
    "qv_qlearn = qlearning.get_qv_func_dict()\n",
    "print(qv_qlearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Test the above algorithms on some example MDPs by using DP Policy Iteration/Value Iteration solutions as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 2.\n",
      "Number of iterations: 98.\n",
      "Value function estimates with policy iteration\n",
      "{1: 34.7221052631579, 2: 35.90210526315789, 3: 0.0}\n",
      "Value function estimates with value iteration\n",
      "{1: 34.72210522497511, 2: 35.90210522497511, 3: 0.0}\n",
      "Value function estimates with Sarsa\n",
      "{1: 29.01505207860538, 2: 29.128953572925912, 3: 0.0}\n",
      "Value function estimates with Q-learning\n",
      "{1: 35.42292975972827, 2: 34.068557603582164, 3: 0.0}\n"
     ]
    }
   ],
   "source": [
    "policy_data = {\n",
    "        1: {'a': 0.4, 'b': 0.6},\n",
    "        2: {'a': 0.7, 'c': 0.3},\n",
    "        3: {'b': 1.0}\n",
    "    }\n",
    "pol1,val1 = mdp_ref_obj1.policy_iteration(policy_data)\n",
    "pol2,val2 = mdp_ref_obj1.value_iteration()\n",
    "print(\"Value function estimates with policy iteration\")\n",
    "print(val1)\n",
    "print(\"Value function estimates with value iteration\")\n",
    "print(val2)\n",
    "val_sarsa = {s:max(v[a] for a in v) for s,v in qv_sarsa.items()}\n",
    "print(\"Value function estimates with Sarsa\")\n",
    "print(val_sarsa)\n",
    "val_qlearn = {s:max(v[a] for a in v) for s,v in qv_qlearn.items()}\n",
    "print(\"Value function estimates with Q-learning\")\n",
    "print(val_qlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
