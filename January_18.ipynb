{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Dynamic Programming Algorithms\n",
    "\n",
    "### Policy Evaluation (tabular) algorithm\n",
    "**1. Problem: ** evaluate a given policy $\\pi$\n",
    "\n",
    "**2. Using synchronous backups: ** \n",
    "\n",
    "   (1) At each iteration $k+1$\n",
    "   \n",
    "   (2) For all states $s \\in S$\n",
    "   \n",
    "   (3) Update $v_{k+1}(s)$ from $v_k(s')$\n",
    "   \n",
    "   (4) where $s'$ is a successor state of $s$\n",
    "\n",
    "**3. Updating value functions: **\n",
    "\n",
    "$v _ { k + 1 } ( s ) = \\sum _ { a \\in \\mathcal { A } } \\pi ( a | s ) \\left( \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { k } \\left( s ^ { \\prime } \\right) \\right)$\n",
    "\n",
    "$\\mathbf { v } ^ { k + 1 } = \\mathcal { R } ^ { \\pi } + \\gamma \\boldsymbol { P } ^ { \\pi } \\mathbf { v } ^ { k }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. An example: ** walking in a small grid\n",
    "<img src=\"small_grid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution of the stable value function of a MDP given a policy: \n",
      "State 1 has value of -14.00.\n",
      "State 2 has value of -20.00.\n",
      "State 3 has value of -22.00.\n",
      "State 4 has value of -14.00.\n",
      "State 5 has value of -18.00.\n",
      "State 6 has value of -20.00.\n",
      "State 7 has value of -20.00.\n",
      "State 8 has value of -20.00.\n",
      "State 9 has value of -20.00.\n",
      "State 10 has value of -18.00.\n",
      "State 11 has value of -14.00.\n",
      "State 12 has value of -22.00.\n",
      "State 13 has value of -20.00.\n",
      "State 14 has value of -14.00.\n"
     ]
    }
   ],
   "source": [
    "from src.mdp import MDP\n",
    "\n",
    "mdp_data = {\n",
    "        0: {\n",
    "            'up': {0: 1},\n",
    "            'down': {0: 1},\n",
    "            'left': {0: 1},\n",
    "            'right': {0: 1}\n",
    "        },\n",
    "        1: {\n",
    "            'up': {1: 1},\n",
    "            'down': {5: 1},\n",
    "            'left': {0: 1},\n",
    "            'right': {2: 1}\n",
    "        },\n",
    "        2: {\n",
    "            'up': {2: 1},\n",
    "            'down': {6: 1},\n",
    "            'left': {1: 1},\n",
    "            'right': {3: 1}\n",
    "        },\n",
    "        3: {\n",
    "            'up': {3: 1},\n",
    "            'down': {7: 1},\n",
    "            'left': {2: 1},\n",
    "            'right': {3: 1}\n",
    "        },\n",
    "        4: {\n",
    "            'up': {0: 1},\n",
    "            'down': {8: 1},\n",
    "            'left': {4: 1},\n",
    "            'right': {5: 1}\n",
    "        },\n",
    "        5: {\n",
    "            'up': {1: 1},\n",
    "            'down': {9: 1},\n",
    "            'left': {4: 1},\n",
    "            'right': {6: 1}\n",
    "        },\n",
    "        6: {\n",
    "            'up': {2: 1},\n",
    "            'down': {10: 1},\n",
    "            'left': {5: 1},\n",
    "            'right': {7: 1}\n",
    "        },\n",
    "        7: {\n",
    "            'up': {3: 1},\n",
    "            'down': {11: 1},\n",
    "            'left': {6: 1},\n",
    "            'right': {7: 1}\n",
    "        },\n",
    "        8: {\n",
    "            'up': {4: 1},\n",
    "            'down': {12: 1},\n",
    "            'left': {8: 1},\n",
    "            'right': {9: 1}\n",
    "        },\n",
    "        9: {\n",
    "            'up': {5: 1},\n",
    "            'down': {13: 1},\n",
    "            'left': {8: 1},\n",
    "            'right': {10: 1}\n",
    "        },\n",
    "        10: {\n",
    "            'up': {6: 1},\n",
    "            'down': {14: 1},\n",
    "            'left': {9: 1},\n",
    "            'right': {11: 1}\n",
    "        },\n",
    "        11: {\n",
    "            'up': {7: 1},\n",
    "            'down': {0: 1},\n",
    "            'left': {10: 1},\n",
    "            'right': {11: 1}\n",
    "        },\n",
    "        12: {\n",
    "            'up': {8: 1},\n",
    "            'down': {12: 1},\n",
    "            'left': {12: 1},\n",
    "            'right': {13: 1}\n",
    "        },\n",
    "        13: {\n",
    "            'up': {9: 1},\n",
    "            'down': {13: 1},\n",
    "            'left': {12: 1},\n",
    "            'right': {14: 1}\n",
    "        },\n",
    "        14: {\n",
    "            'up': {10: 1},\n",
    "            'down': {14: 1},\n",
    "            'left': {13: 1},\n",
    "            'right': {0: 1}\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "reward = {}\n",
    "for state in mdp_data.keys():\n",
    "    reward[state] = {}\n",
    "    for action in mdp_data[state].keys():\n",
    "        if state == 0:\n",
    "            reward[state][action] = 0\n",
    "        else:\n",
    "            reward[state][action] = -1\n",
    "        \n",
    "policy_data = {}\n",
    "for state in mdp_data.keys():\n",
    "    policy_data[state] = {}\n",
    "    for action in mdp_data[state].keys():\n",
    "        policy_data[state][action] = 0.25\n",
    "        \n",
    "gamma = 1\n",
    "\n",
    "mdp = MDP(mdp_data, reward, gamma)\n",
    "mrp = mdp.get_mrp(policy_data)\n",
    "print(\"Solution of the stable value function of a MDP given a policy: \")\n",
    "for k,v in mdp.policy_evaluation(policy_data).items():\n",
    "    print(\"State {} has value of {:2.2f}.\".format(k,v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (tabular) algorithm\n",
    "\n",
    "**1. Method: ** evaluate and imporve policy $\\pi_k$\n",
    "\n",
    "    (1) Evaluate the policy.\n",
    "$$v _ { \\pi } ( s ) = \\mathbb { E } \\left[ R _ { t + 1 } + \\gamma R _ { t + 2 } + \\ldots | S _ { t } = s \\right]$$\n",
    "\n",
    "    (2) Improve the policy by acting greedily\n",
    "$$\\pi ^ { \\prime } = \\operatorname { greedy } \\left( v _ { \\pi } \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 3.\n",
      "Optimal policy: \n",
      "State 0: go up. Value: 0\n",
      "State 1: go left. Value: -1.0\n",
      "State 2: go left. Value: -2.0\n",
      "State 3: go down. Value: -3.0\n",
      "State 4: go up. Value: -1.0\n",
      "State 5: go up. Value: -2.0\n",
      "State 6: go up. Value: -3.0\n",
      "State 7: go down. Value: -2.0\n",
      "State 8: go up. Value: -2.0\n",
      "State 9: go up. Value: -3.0\n",
      "State 10: go down. Value: -2.0\n",
      "State 11: go down. Value: -1.0\n",
      "State 12: go up. Value: -3.0\n",
      "State 13: go right. Value: -2.0\n",
      "State 14: go right. Value: -1.0\n"
     ]
    }
   ],
   "source": [
    "pol_opt, val_opt = mdp.policy_iteration(policy_data, 20)\n",
    "print(\"Optimal policy: \")\n",
    "for state in mdp_data.keys():\n",
    "    print(\"State {}: go {}. Value: {}\".format(state, \\\n",
    "          [k for k in pol_opt[state].keys()][0], val_opt[state]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration (tabular) algorithm\n",
    "\n",
    "**1. Problem: ** find optimal policy  $\\pi$\n",
    "\n",
    "**2. Solution: ** iterative application of Bellman optimality backup\n",
    "$$\n",
    "v _ { 1 } \\rightarrow v _ { 2 } \\rightarrow \\ldots \\rightarrow v _ { * }\n",
    "$$\n",
    "$$\n",
    "v _ { k + 1 } ( s ) = \\max _ { a \\in \\mathcal { A } } \\left( \\mathcal { R } _ { s } ^ { a } + \\gamma \\sum _ { s ^ { \\prime } \\in \\mathcal { S } } \\mathcal { P } _ { s s ^ { \\prime } } ^ { a } v _ { k } \\left( s ^ { \\prime } \\right) \\right)\n",
    "$$\n",
    "$$\n",
    "\\mathbf { v } _ { k + 1 } = \\max _ { a \\in \\mathcal { A } } \\mathcal { R } ^ { a } + \\gamma \\mathcal { P } ^ { a } \\mathbf { v } _ { k }\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 4.\n",
      "Optimal policy: \n",
      "State 0: go up. Value: 0\n",
      "State 1: go left. Value: -1\n",
      "State 2: go left. Value: -2\n",
      "State 3: go down. Value: -3\n",
      "State 4: go up. Value: -1\n",
      "State 5: go up. Value: -2\n",
      "State 6: go up. Value: -3\n",
      "State 7: go down. Value: -2\n",
      "State 8: go up. Value: -2\n",
      "State 9: go up. Value: -3\n",
      "State 10: go down. Value: -2\n",
      "State 11: go down. Value: -1\n",
      "State 12: go up. Value: -3\n",
      "State 13: go right. Value: -2\n",
      "State 14: go right. Value: -1\n"
     ]
    }
   ],
   "source": [
    "pol_opt, val_opt = mdp.value_iteration()\n",
    "print(\"Optimal policy: \")\n",
    "for state in mdp_data.keys():\n",
    "    print(\"State {}: go {}. Value: {}\".format(state, \\\n",
    "          [k for k in pol_opt[state].keys()][0], val_opt[state]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
